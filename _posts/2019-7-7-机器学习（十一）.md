---
layout:     post
title:      机器学习（十一）
subtitle:   训练深度神经网络
date:       2019-6-10
author:     Gavin
header-img: img/post-bg-ioses.jpg
catalog: true
tags:
    - Machine Learning
    - Data Operation
---

> 山桃红花满上头
> 
> 蜀江春水拍山流

# 前言

#### 介绍

之前我们训练了第一个深度学习模型，但那是一个非常浅的深度神经网络，只有两个隐藏层。如果是一个更复杂的情况呢？比如有10层或者包含成百上千个神经元并切连接众多。这样的话，也许就会出现以下情况：  

* **梯度消失**或**梯度爆炸**，导致低层很难训练
* 如此巨大的神经网络，训练十分缓慢
* 一个拥有如此之多的参数的模型过拟合风险极高

我们将学习如何解决这些问题

---

# 梯度消失／爆炸问题

#### 定义

我们知道，神经网络利用反向传播算法得到代价函数的梯度，并利用这些梯度更新权重。但是，梯度往往越到低层变得越来越小，结果，叶结点的权重根本不改变，训练也始终达不到理想效果，这是*梯度消失*。  
有些案例中，梯度反而会越变越大，很多层的权重变得极端大，导致最后算法发散了，这是*梯度爆炸*。  
更普遍的情况是，深度神经网络苦于梯度震荡，各层以不同的速率学习。  
![](http://ww2.sinaimg.cn/large/006tNc79ly1g5ukly8v3tj30fz07ptbz.jpg)  
由上图逻辑激活函数可知，当输入增大（正或负）时，函数在0、1饱和，导致导数基本为0，那么反向传播时，基本没作用。  

#### Xavier初始化和He初始化

我们需要让信号在两个方向上（预测和反向传播）流动，同时我们不希望信号消亡或者爆炸，因此有学者提出为了实现这一点，可以保持每一层输入和输出的方差一致，且反向传播时，方差也一致。因此提出了*Xavier初始化*，即对每一层的连接权重进行初始化，分布参数如下。  
![](http://ww4.sinaimg.cn/large/006tNc79ly1g5ukzvdtvbj30b004kgm6.jpg)  

#### 非饱和激活函数

ReLU函数往往会出现“死亡”问题，即很多节点只输出0。因此就可能需要使用ReLU的变种，如*leaky ReLU*。或者使用其他激活函数，如*ELU（加速线性单元）*。通常来说：ELU > leak ReLU > ReLU > tanh > 逻辑函数。  

leak ReLU：  

```
def leak_relu(z, name=None):
	return tf.maximum(0.01*z,z,name=name)
	
hidden = fully_connected(X,n_hidden,activation_fn=leak_relu)
```

#### 批量归一化

```
bn_params = {
	'is_training': true,
	'decay': 0.99,
	'updates_collections': None
}
hidden = fully_connected(X,n_hidden,normalizer_fn=batch_norm,normalizer_params=bn_params)
```  

#### 梯度裁剪

```
threshold = 1.0
optimizer = tf.train.GradientDescentOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(loss)
capped_gvs = [(tf.clip_by_value(grad,-threshold,threshold),var) for grad,var in grads_and_vars]
training_op = optimizer.apply_gradients(capped_gvs)
```  

#### 重用训练图层